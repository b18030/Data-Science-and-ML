{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Deep Learning Neurons__\n",
    "![title](https://miro.medium.com/max/875/1*T4ARzySpEQvEnr_9pc78pg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplified models of biological neurons, as described above, can be __assembled to form the stereotypical neuron in deep learning models.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The simplified models of biological neurons, as described above, can be assembled to form the stereotypical neuron in deep learning models\n",
    "- The activations are multiplied by synaptic weights. These weights are models of synaptic strengths in biological neurons, and also model inhibitory transmisssion, in that the weights may take on negative values.\n",
    "- The weighted activations are summed together, modeling the accumulation process that happens in the cell body of a biological neuron.\n",
    "- A bias term is added to the sum, modeling the general sensitivity of neuron.\n",
    "-Finally, the summed value is shaped by an activation function — typically one that limits the minimum or maximum output value (or both), such as a sigmoid function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://blog.adverai.com/wp-content/uploads/2018/06/Blog-illustrations-1@2x-1-768x532.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __In neural networks, we can only have real numbers as inputs.__\n",
    "![title](https://blog.adverai.com/wp-content/uploads/2018/06/Blog-illustrations-2@2x-1-768x524.jpg)\n",
    ".            Representation of a node. __The darker the arrow the higher the weight of the input__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron\n",
    "- Perceptron is a __single layer neural network__ and a multi-layer perceptron is called Neural Networks.\n",
    "- Perceptron is a __linear classifier (binary)__. Also, it is used in supervised learning. It helps to classify the given input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/1250/1*c1__xa2EPo4xDLBXGG3uMw.png)\n",
    "\n",
    " __To make a prediction__ from above calculated score, we have to use an __activation function__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why do we need Weights and Bias?__\n",
    "> __Weights__ shows the strength of the particular node.\n",
    "> A __bias__ value allows you to shift the activation function curve up or down.\n",
    "\n",
    "- __Why do we need Activation Function?__\n",
    "> In short, the activation functions are used to map the input between the required values like (0, 1) or (-1, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid or Logistic Activation Function__\n",
    "The Sigmoid Function curve looks like a S-shape.\n",
    "![title](https://miro.medium.com/max/606/1*Xu7B5y9gp0iL5ooBj7LtWw.png)\n",
    "\n",
    "The __main reason why we use sigmoid function__ is because it exists between (0 to 1). Therefore, it is especially __used for models where we have to predict the probability as an output__.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surprisal:\n",
    "__“Degree to which you are surprised to see the result”__\n",
    " <br>I will be more surprised to see an outcome with low probability in comparison to an outcome with high probability. Now, if yi is the probability of ith outcome then we could represent surprisal (s) as:\n",
    " ![title](https://miro.medium.com/max/141/1*pPaLYXQ2UTPMiyyHtS1FTQ.png)\n",
    " \n",
    " #### Entropy:\n",
    " Weighted average of surprisal is nothing but Entropy (e) and if there are n outcomes then it could be written as:\n",
    " ![title](https://miro.medium.com/max/195/1*GiVsClwByV9v-8mAVGdRBg.png)\n",
    " \n",
    " ### Cross-Entropy:\n",
    " Now, what if each outcome’s __actual probability is pi__ but someone is __estimating probability as qi__. In this case, each event will occur with the probability of pi but surprisal will be given by qi in its formula (since that person will be surprised thinking that probability of the outcome is qi). \n",
    " ![title](https://miro.medium.com/max/199/1*VqRZI4pE-chjDHYuI5d-cw.png)\n",
    "> __Minimizing cross entropy will move us closer to actual/desired distribution and that is what we want. This is why we try to reduce cross entropy so that our predicted probability distribution end up being close to the actual one.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So using ___Gradient Descent Update Rule___\n",
    "   \n",
    "   <img src=\"images/grad_for_binary_cross_entropy_loss.jpeg\" alt=\"drawing\" style=\"width:490px;\"/>\n",
    "   <img src=\"images/grad_for_binary_cross_entropy_loss2.jpeg\" alt=\"drawing\" style=\"width:190px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Epoch__ is the number of times we’ve iterated through the entire training set.\n",
    "- __Threshold is the maximum number of epoch we will allow to pass while training__. There __is not built in stopping point of our algorithm__. It will continue adding 0 to our weights, on and on, forever. __Adding a threshold is one way of stopping our training loop__.\n",
    "- __Learning rate__, is the magnitude at which we increase or decrease our weights during each iteration of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
