{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Come to Multilayer Perceptron(MLP) or Feed Forward Neural Network(FFNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/1400/1*eloYEyFrblGHVZhU345PJw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hidden Layers, which are neuron nodes stacked in between inputs and outputs, allowing neural networks to learn more complicated features__\n",
    "> __An MLP can be thought of, therefore, as a deep artificial neural network.\n",
    "\n",
    "![title](https://miro.medium.com/max/875/1*gFr5bUiL2LzWFpKYTQ_c6Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ___Training involves adjusting the parameters__, or the weights and biases, of the model in order to minimize error. Backpropagation is used to make those weigh and bias adjustments relative to the error, and the error itself can be measured in a variety of ways, including by root mean squared error (RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/max/625/1*yJM9Feo4pPk_Bj8FHm62Vg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In the backward pass, using backpropagation and the chain rule of calculus, partial derivatives of the error function regarding the various weights and biases are back-propagated through the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-1 Implementation\n",
    "- Neural network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = 2 # no_of_features\n",
    "layers=[4,3]# no. of neurons in first and second layer\n",
    "output_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,layers,output_size):\n",
    "        np.random.seed(0)\n",
    "        model={}#Dictionary\n",
    "        \n",
    "        # First Hidden Layer\n",
    "        model['W1'] = np.random.randn(input_size,layers[0])\n",
    "        model['b1']=np.zeros((1,layers[0]))\n",
    "        \n",
    "        # Second Hidden Layer\n",
    "        model['W2'] = np.random.randn(layers[0],layers[1])\n",
    "        model['b2']=np.zeros((1,layers[0]))\n",
    "        \n",
    "        # Output  Layer\n",
    "        model['W3'] = np.random.randn(layers[1],output_size)\n",
    "        model['b3']=np.zeros((1,layers[0]))\n",
    "        \n",
    "        self.model=model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider an L layer neural network, which has L-1 hidden layers and 1 output layer. The parameters (weights and biases) of the layer l are represented as\n",
    "![](https://miro.medium.com/max/875/1*UbPOjOD6IATWM2VDMA3JBg.png)\n",
    "\n",
    "#### __Initializing weights W__\n",
    "- One of the starting points to take care of while building your network is to initialize your weight matrix correctly\n",
    "    - __Initializing all weights to 0__\n",
    "        - this makes your model equivalent to a linear model. When you set all weight to 0,all the weights have the same values in the subsequent iteration. This makes the hidden units symmetric and continues for all the n iterations you run\n",
    "        ![](https://miro.medium.com/max/563/1*_wS_ul0act9fCT-b7SuONQ.png)\n",
    "        If we set all the weights to be zero, __then all the the neurons of all the layers performs the same calculation, giving the same output and there by making the whole deep net useless__\n",
    "    - Initializing weights randomly<br>\n",
    "        - This serves the __process of symmetry-breaking and gives much better accuracy__\n",
    "        - However Initializing weights randomly, following standard normal distribution  while working with a (deep) network can potentially lead to 2 issues â€” vanishing gradients or exploding gradients.\n",
    "        ![title](https://miro.medium.com/max/350/1*3xDSB2RxoKDPnrdsonV4aA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Here below the Weight Matrix__:\n",
    "\n",
    "![](https://miro.medium.com/max/875/1*1JCfvimyyFm1yqWMYmnm2g.png)\n",
    "#### As you can see in the image, the input layer has 3 neurons and the very next layer (a hidden layer) has 4. We can create a matrix of 3 rows and 4 columns and insert the values of each weight in the matrix as done above. This matrix would be called W1. In the case where we have more layers, we would have more weight matrices, W2, W3, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
